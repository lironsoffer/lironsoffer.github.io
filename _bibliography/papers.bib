---
---

@inproceedings{shani2025tokens,
  abbr        = {ICLR},
  title       = {From Tokens to Thoughts: How {LLMs} and Humans Trade Compression for Meaning},
  author      = {Shani, Chen and Soffer, Liron and Jurafsky, Dan and LeCun, Yann and Shwartz-Ziv, Ravid},
  booktitle   = {The Thirteenth International Conference on Learning Representations},
  year        = {2026},
  url         = {https://arxiv.org/abs/2505.17117},
  html        = {https://arxiv.org/abs/2505.17117},
  arxiv       = {2505.17117},
  abstract    = {We apply an Information Bottleneck framework to examine how LLMs handle the trade-off between compression and semantic meaning, comparing them against human conceptual structures. LLMs align broadly with human category boundaries but lack fine-grained semantic distinctions; they aggressively compress information whereas humans maintain representations preserving contextual nuance; encoder models outperform larger decoder models in human alignment; and training dynamics reveal a two-phase trajectory where models shift from rapid concept formation to architectural reorganization.},
  selected    = {true},
  bibtex_show = {true}
}
